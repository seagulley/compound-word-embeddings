{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates a training set for IMDb pretraining\n",
    "- Each row is of the generated .csv is a review, with each word represented with the BERT embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kalliehuynh/miniforge3/envs/stenv/lib/python3.8/site-packages/jax/_src/lib/__init__.py:34: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kalliehuynh/miniforge3/envs/stenv/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kalliehuynh/miniforge3/envs/stenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = pd.read_csv('BERT_embeddings.csv', index_col=False)\n",
    "bert_embeddings = bert_embeddings.drop(['Unnamed: 0'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/kalliehuynh/compound-word-embeddings/data_processing/aclImdb/test/neg/3_4.txt', 'r') as file:\n",
    "    file_str = file.read()\n",
    "    tokens = tokenizer.tokenize(file_str)\n",
    "    tokens = tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_array(tokens):\n",
    "    \"\"\"Generates an array of embeddings for a given tokenized piece of text.\n",
    "    Truncate the list of tokens to 100.\n",
    "    If there are less than 100 tokens, pad the array with 0's.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): A list of tokens\n",
    "\n",
    "    Returns:\n",
    "        numpy.array: An array of embeddings, padded and truncated to a shape of (768, 100)\n",
    "    \"\"\"\n",
    "    tokens = tokens[:100]\n",
    "    embeddings_list = []\n",
    "    for token in tokens:\n",
    "        if not bert_embeddings.loc[bert_embeddings['word']==token].empty:\n",
    "            embedding = bert_embeddings.loc[bert_embeddings['word']==token].iloc[:, 1:].to_numpy()\n",
    "            embeddings_list.append(embedding)\n",
    "    embeddings_array = np.array(embeddings_list)\n",
    "    num_embeddings, _, _ = embeddings_array.shape\n",
    "    embeddings_array = np.pad(embeddings_array, [(0, 100-num_embeddings), (0, 0), (0, 0)], 'constant')\n",
    "    \n",
    "    return embeddings_array.tolist()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg = []\n",
    "for root_dir_path, sub_dirs, files in os.walk('/Users/kalliehuynh/compound-word-embeddings/data_processing/aclImdb/test/neg'):\n",
    "    for file in files:\n",
    "        with open(root_dir_path + '/' + file) as f:\n",
    "            test_neg.append(review_array(tokenizer.tokenize(f.read())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 12500\n",
      "Number of words per review: 100\n",
      "Length of each word vector: 1\n"
     ]
    }
   ],
   "source": [
    "# Double-check dimensions of the list:\n",
    "print('Number of reviews:', len(test_neg))\n",
    "print('Number of words per review:', len(test_neg[0]))\n",
    "print('Length of each word vector:',  len(test_neg[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_labels = ['word_%02d' % x for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame(columns=[*array_labels])\n",
    "for i in range(len(test_neg)):\n",
    "    test_data.loc[i, array_labels] = test_neg[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['positive'] = [0] * len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test_neg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('stenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9c1e931f97bf0988d4b0e08b56fbd4208c7bcd528771344c324ad3f2e1513e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
